{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM for Sentiment Analysis",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r67y9UpchZ38"
      },
      "source": [
        "# Recurrent Neural Networks\n",
        "\n",
        "- 給定一個語句，判斷他有沒有惡意（負面標 1，正面標 0）\n",
        "- 參考李弘毅老師ML課程[HW4](https://colab.research.google.com/drive/16d1Xox0OW-VNuxDn1pvy2UXFIPfieCb9#scrollTo=ICDIhhgCY2-M)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBNRJLXCRikp"
      },
      "source": [
        "### Change Working Dir & Some Initial Configure"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83frvkIFijDO",
        "outputId": "a3f1fdcb-9b82-4597-df4f-3d25c686796e"
      },
      "source": [
        "import os\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path_prefix = '/content/drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network'\n",
        "# path_prefix = './'\n",
        "# %cd '/content/drive/My Drive/Colab Notebooks/hw4 - Recurrent Neural Network/'\n",
        "# %cd C:\\Users\\a5465\\Desktop\\side-project\\ChatBot\n",
        "os.getcwd()\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'C:\\\\Users\\\\a5465\\\\desktop\\\\side-project\\\\ChatBot'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hDIokoP6464"
      },
      "source": [
        "# this is for filtering the warnings\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# path prefix setting\n",
        "model_prefix = './model'\n",
        "if not os.path.exists('./model'):\n",
        "  os.mkdir('./model')\n",
        "path_prefix = './'\n",
        "data_prefix = './data'\n",
        "\n",
        "embd_dim = 250"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc143hSvNGr6"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICDIhhgCY2-M"
      },
      "source": [
        "# # utils.py\n",
        "# # 這個 block 用來先定義一些等等常用到的函式\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch.optim as optim\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# def load_training_data(path=train_x_path):\n",
        "#     # 把 training 時需要的 data 讀進來\n",
        "#     # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\n",
        "#     if 'KaoKao' in path:\n",
        "#         with open(path, 'r', encoding='utf8') as f:\n",
        "#           lines = f.readlines()\n",
        "#           lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "#         x = [line[2:] for line in lines]\n",
        "#         for i in range(len(x)):\n",
        "#           while True:\n",
        "#             try:\n",
        "#               x[i].remove('')\n",
        "#             except:\n",
        "#               break\n",
        "\n",
        "#         y = [line[0] for line in lines]\n",
        "#         return x, y\n",
        "#     # else:\n",
        "#     #     with open(path, 'r') as f:\n",
        "#     #         lines = f.readlines()\n",
        "#     #         x = [line.strip('\\n').split(' ') for line in lines]\n",
        "#     #     return x\n",
        "\n",
        "# def load_testing_data(path='testing_data'):\n",
        "#     # 把 testing 時需要的 data 讀進來\n",
        "#     with open(path, 'r') as f:\n",
        "#         lines = f.readlines()\n",
        "#         X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
        "#         X = [sen.split(' ') for sen in X]\n",
        "#     return X\n",
        "\n",
        "# def evaluation(outputs, labels):\n",
        "#     # outputs => probability (float)\n",
        "#     # labels => labels\n",
        "#     # print('outputs.shape: {}'.format(outputs.shape))\n",
        "#     # print('labels.shape: {}'.format(labels.shape))\n",
        "#     # outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
        "#     # outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
        "#     outputs_class = torch.zeros(labels.shape[0]).to(device)\n",
        "#     for i in range(0, labels.shape[0]):\n",
        "#       max_idx = 0\n",
        "#       for j in range(1, outputs[i].shape[0]):\n",
        "#         if outputs[i,j] > outputs[i,max_idx]:\n",
        "#           max_idx = j\n",
        "#       outputs_class[i] = max_idx\n",
        "#     correct = torch.sum(torch.eq(outputs_class, labels)).item()\n",
        "#     return correct"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYdC1WOpL59J"
      },
      "source": [
        "# utils.py\n",
        "# 這個 block 用來先定義一些等等常用到的函式\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import jieba\n",
        "\n",
        "def load_training_data():\n",
        "    # load positive sentences\n",
        "    lines_cut = []\n",
        "    labels = []\n",
        "    with open(os.path.join(data_prefix, 'positive.txt').replace('\\\\','/'), 'r', encoding='utf8') as f:\n",
        "      lines = f.readlines()\n",
        "      lines = [line.strip('\\n') for line in lines]\n",
        "      for line in lines:\n",
        "        jieba_cut = jieba.cut(line)\n",
        "        line_cut = []\n",
        "        for word in jieba_cut:\n",
        "          line_cut.append(word)\n",
        "        lines_cut.append(line_cut)\n",
        "        labels.append(1)\n",
        "\n",
        "    # load negative sentences\n",
        "    with open(os.path.join(data_prefix, 'negative.txt').replace('\\\\','/'), 'r', encoding='utf8') as f:\n",
        "      lines = f.readlines()\n",
        "      lines = [line.strip('\\n') for line in lines]\n",
        "      for line in lines:\n",
        "        jieba_cut = jieba.cut(line)\n",
        "        line_cut = []\n",
        "        for word in jieba_cut:\n",
        "          line_cut.append(word)\n",
        "        lines_cut.append(line_cut)\n",
        "        labels.append(0)\n",
        "\n",
        "    with open(os.path.join(data_prefix, 'ntusd-negative.txt').replace('\\\\','/'), 'r', encoding='utf8') as f:\n",
        "      lines = f.readlines()\n",
        "      lines = [line.strip('\\n') for line in lines]\n",
        "      for line in lines:\n",
        "        jieba_cut = jieba.cut(line)\n",
        "        line_cut = []\n",
        "        for word in jieba_cut:\n",
        "          line_cut.append(word)\n",
        "        lines_cut.append(line_cut)\n",
        "        labels.append(2)\n",
        "\n",
        "\n",
        "    return lines_cut, labels\n",
        "\n",
        "def loading_corpus:\n",
        "# def load_testing_data(path='testing_data'):\n",
        "#     # 把 testing 時需要的 data 讀進來\n",
        "#     with open(path, 'r') as f:\n",
        "#         lines = f.readlines()\n",
        "#         X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n",
        "#         X = [sen.split(' ') for sen in X]\n",
        "#     return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    # print('outputs.shape: {}'.format(outputs.shape))\n",
        "    # print('labels.shape: {}'.format(labels.shape))\n",
        "    # outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\n",
        "    # outputs[outputs<0.5] = 0 # 小於 0.5 為負面\n",
        "    outputs_class = torch.zeros(labels.shape[0]).to(device)\n",
        "    for i in range(0, labels.shape[0]):\n",
        "      max_idx = 0\n",
        "      for j in range(1, outputs[i].shape[0]):\n",
        "        if outputs[i,j] > outputs[i,max_idx]:\n",
        "          max_idx = j\n",
        "      outputs_class[i] = max_idx\n",
        "    correct = torch.sum(torch.eq(outputs_class, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYE8UYQsNIxM"
      },
      "source": [
        "### Train Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgGWaF8_2S3q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55c85e0e-df62-448e-e66a-20935eefc048"
      },
      "source": [
        "# w2v.py\n",
        "# 這個 block 是用來訓練 word to vector 的 word embedding\n",
        "# 注意！這個 block 在訓練 word to vector 時是用 cpu，可能要花到 10 分鐘以上\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    # 訓練 word to vector 的 word embedding\n",
        "    model = word2vec.Word2Vec(x, corpus_file=x, size=embd_dim, window=5, min_count=5, workers=12, iter =10, sg=1)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data()\n",
        "    # train_x_no_label = load_training_data('training_nolabel.txt')\n",
        "\n",
        "    # print(\"loading testing data ...\")\n",
        "    # test_x = load_testing_data('testing_data.txt')\n",
        "\n",
        "    #model = train_word2vec(train_x + train_x_no_label + test_x)\n",
        "    model = train_word2vec(train_x)\n",
        "    \n",
        "    print(\"saving model ...\")\n",
        "    # model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\n",
        "    model.save(os.path.join(model_prefix, 'w2v_all-NTCH.model').replace('\\\\','/'))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading training data ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-76-61664c87c923>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#model = train_word2vec(train_x + train_x_no_label + test_x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ntusd-positive.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saving model ...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-76-61664c87c923>\u001b[0m in \u001b[0;36mtrain_word2vec\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# 訓練 word to vector 的 word embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0membd_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\a5465\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, max_final_vocab)\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2VecTrainables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashfxn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhashfxn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m         super(Word2Vec, self).__init__(\n\u001b[0m\u001b[0;32m    598\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\a5465\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, sentences, corpus_file, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, ns_exponent, cbow_mean, min_alpha, compute_loss, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m             self.train(\n\u001b[0m\u001b[0;32m    747\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\a5465\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         \"\"\"\n\u001b[1;32m--> 724\u001b[1;33m         return super(Word2Vec, self).train(\n\u001b[0m\u001b[0;32m    725\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\a5465\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1061\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_training_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1063\u001b[1;33m         return super(BaseWordEmbeddingsModel, self).train(\n\u001b[0m\u001b[0;32m   1064\u001b[0m             \u001b[0mdata_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1065\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\a5465\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m         self._check_training_sanity(\n\u001b[0m\u001b[0;32m    533\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\a5465\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1173\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhmZXPTFpFBo",
        "outputId": "036974aa-5e2f-4352-dd63-d85a7fbc2466"
      },
      "source": [
        "print(train_x[-5:])\n",
        "print(y[-5:])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['噁', '心'], ['噁', '心鬼'], ['憋死'], ['憋', '著'], ['蹩腳']]\n",
            "[2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXXywgzNzSwz",
        "outputId": "6e5b28f3-9bb1-4efc-a35b-b6baf4b73702"
      },
      "source": [
        "sen_max = 0\n",
        "sum = 0\n",
        "for sen in train_x:\n",
        "  sum += len(sen)\n",
        "  # print(sen)\n",
        "  if len(sen) > sen_max:\n",
        "    sen_max = len(sen)\n",
        "sen_avg = sum/len(train_x)\n",
        "print('senLen_max: {}'.format(sen_max))\n",
        "print('senLen_avg: {}'.format(sen_avg))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "senLen_max: 8710\n",
            "senLen_avg: 32.7176577443838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wHLtS0wNR6w"
      },
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfGKiOitk5ob"
      },
      "source": [
        "# preprocess.py\n",
        "# 這個 block 用來做 data 的預處理\n",
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sentences = sentences\n",
        "        self.sen_len = sen_len\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # 把之前訓練好的 word to vec 模型讀進來\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word):\n",
        "        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\n",
        "        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        # 取得訓練好的 Word2vec word embedding\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 製作一個 word2idx 的 dictionary\n",
        "        # 製作一個 idx2word 的 list\n",
        "        # 製作一個 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab):\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            #e.g. self.word2index['he'] = 1 \n",
        "            #e.g. self.index2word[1] = 'he'\n",
        "            #e.g. self.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "    def pad_sequence(self, sentence):\n",
        "        # 將每個句子變成一樣的長度\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "    def sentence_word2idx(self):\n",
        "        # 把句子裡面的字轉成相對應的 index\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            # 將每個句子變成一樣的長度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # 把 labels 轉成 tensor\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJB7go5NWL0"
      },
      "source": [
        "### Custom Dataset Implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XketwKs4lFfB"
      },
      "source": [
        "# data.py\n",
        "# 實作了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
        "# 好讓 dataloader 能使用\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class CustomDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape : (data_num, seq_len, feature_dim)\n",
        "    \n",
        "    __len__ will return the number of data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None: return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNJ8xWIMNa2r"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6RJADulIq1"
      },
      "source": [
        "# model.py\n",
        "# 這個 block 是要拿來訓練的模型\n",
        "import torch\n",
        "from torch import nn\n",
        "class LSTM_Net(nn.Module):\n",
        "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        # 製作 embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
        "                            nn.Linear(hidden_dim, 2),\n",
        "                            nn.Sigmoid() )\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # x 的 dimension (batch, seq_len, hidden_size)\n",
        "        # 取用 LSTM 最後一層的 hidden state\n",
        "        x = x[:, -1, :] \n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWlpEL0sNc10"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QR4MMz-lR7i"
      },
      "source": [
        "# train.py\n",
        "# 這個 block 是用來訓練模型的\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
        "    # total = sum(p.numel() for p in model.parameters())\n",
        "    # trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    # print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\n",
        "    criterion = nn.CrossEntropyLoss().to(device) # 定義損失函數，這裡我們使用 binary cross entropy loss\n",
        "    t_batch = len(train) \n",
        "    v_batch = len(valid) \n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        print('\\nepoch: {}'.format(epoch+1))\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # 這段做 training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\n",
        "            outputs = model(inputs) # 將 input 餵給模型\n",
        "            # outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "            # print('outputs.shape: {}'.format(outputs.shape))\n",
        "            loss = criterion(outputs.double(), labels.long()).to(device) # 計算此時模型的 training loss\n",
        "            # print('outputs: {}'.format(outputs))\n",
        "            # print('labels: {}'.format(labels))\n",
        "            # print('loss: {}'.format(loss.item()))\n",
        "            # input()\n",
        "            loss.backward() # 算 loss 的 gradient\n",
        "            optimizer.step() # 更新訓練模型的參數\n",
        "            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss.item()\n",
        "            # print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size))\n",
        "        print('Train | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
        "\n",
        "        # 這段做 validation\n",
        "        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\n",
        "                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\n",
        "                outputs = model(inputs) # 將 input 餵給模型\n",
        "                # outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\n",
        "                # print('inputs.shape: {}'.format(inputs.shape))\n",
        "                # print('outputs.shape: {}'.format(outputs.shape))\n",
        "                loss = criterion(outputs.double(), labels.long()).to(device) # 計算此時模型的 validation loss\n",
        "                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "            if total_acc > best_acc:\n",
        "                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\n",
        "                best_acc = total_acc\n",
        "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
        "                torch.save(model, \"{}/ckpt-NTCH.model\".format(model_dir))\n",
        "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
        "        print('-----------------------------------------------')\n",
        "        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）\n",
        "\n",
        "    # outputs = model(train.dataset.data.to(device))\n",
        "    # print(outputs)\n",
        "    # return outputs"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF5YQrupNfCS"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X2wkdAYxHYA"
      },
      "source": [
        "# test.py\n",
        "# 這個 block 用來對 testing_data.txt 做預測\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def testing(batch_size, test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            ret_output.append(outputs)\n",
        "\n",
        "    \n",
        "    return ret_output"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfnKj0KXNeoz"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EztIWqCmlZof",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04d7e7cb-0a0e-4b92-a467-e0d5328d7e0f"
      },
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "import argparse\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from gensim.models import word2vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 通過 torch.cuda.is_available() 的回傳值進行判斷是否有使用 GPU 的環境，如果有的話 device 就設為 \"cuda\"，沒有的話就設為 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 處理好各個 data 的路徑\n",
        "# train_with_label = os.path.join(path_prefix, train_data_path)\n",
        "# train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "# testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "\n",
        "w2v_path = os.path.join(model_prefix, 'w2v_all-NTCH.model').replace('\\\\','/') # 處理 word to vec model 的路徑\n",
        "\n",
        "# 定義句子長度、要不要固定 embedding、batch 大小、要訓練幾個 epoch、learning rate 的值、model 的資料夾路徑\n",
        "sen_len = 47\n",
        "fix_embedding = True # fix embedding during training\n",
        "batch_size = 256\n",
        "epoch = 10\n",
        "lr = 0.01\n",
        "# model_dir = os.path.join(path_prefix, 'model/') # model directory for checkpoint model\n",
        "model_dir = model_prefix # model directory for checkpoint model\n",
        "\n",
        "print(\"loading data ...\") # 把 'training_label.txt' 跟 'training_nolabel.txt' 讀進來\n",
        "train_x, y = load_training_data()\n",
        "# train_x_no_label = load_training_data()\n",
        "\n",
        "# 對 input 跟 labels 做預處理\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "# 製作一個 model 的對象\n",
        "model = LSTM_Net(embedding, embedding_dim=embd_dim, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\n",
        "\n",
        "# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\n",
        "\n",
        "df_orgin = pd.DataFrame(np.concatenate((train_x.numpy(), y.numpy().reshape(y.numpy().shape[0],1)), axis=1))\n",
        "df = df_orgin[df_orgin[47] == 0].sample(n=df_orgin[df_orgin[47] == 1].shape[0])\n",
        "df = pd.concat([df, df_orgin[df_orgin[47] == 1]])\n",
        "df = pd.concat([df, df_orgin[df_orgin[47] == 2]])\n",
        "df[47] = df[47].replace(2,0)\n",
        "train, val = train_test_split(df, test_size=0.2, shuffle=True)\n",
        "y_train = train.iloc[:,-1].values\n",
        "y_val = val.iloc[:,-1].values\n",
        "X_train = train.iloc[:,:-1].values\n",
        "X_val = val.iloc[:,:-1].values\n",
        "\n",
        "X_train = torch.from_numpy(X_train).to(device)\n",
        "y_train = torch.from_numpy(y_train).to(device)\n",
        "X_val = torch.from_numpy(X_val).to(device)\n",
        "y_val = torch.from_numpy(y_val).to(device)\n",
        "\n",
        "# train_num = 30000\n",
        "# X_train, X_val, y_train, y_val = train_x[:train_num], train_x[train_num:], y[:train_num], y[train_num:]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 取用\n",
        "train_dataset = CustomDataset(X=X_train, y=y_train)\n",
        "val_dataset = CustomDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 轉成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
        "                        # num_workers = 2)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(dataset = val_dataset, batch_size = batch_size, shuffle = False)\n",
        "                        # num_workers = 8)\n",
        "\n",
        "# 開始訓練\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)\n",
        "\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #52287\n",
            "total words: 52289\n",
            "\n",
            "epoch: 1\n",
            "Train | Loss:0.50410 Acc: 80.171\n",
            "Valid | Loss:0.43273 Acc: 87.580 \n",
            "saving model with acc 87.580\n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 2\n",
            "Train | Loss:0.43509 Acc: 87.494\n",
            "Valid | Loss:0.41940 Acc: 89.045 \n",
            "saving model with acc 89.045\n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 3\n",
            "Train | Loss:0.41973 Acc: 89.043\n",
            "Valid | Loss:0.42128 Acc: 88.749 \n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 4\n",
            "Train | Loss:0.41589 Acc: 89.423\n",
            "Valid | Loss:0.41669 Acc: 89.300 \n",
            "saving model with acc 89.300\n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 5\n",
            "Train | Loss:0.41552 Acc: 89.506\n",
            "Valid | Loss:0.41499 Acc: 89.471 \n",
            "saving model with acc 89.471\n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 6\n",
            "Train | Loss:0.41318 Acc: 89.725\n",
            "Valid | Loss:0.41406 Acc: 89.491 \n",
            "saving model with acc 89.491\n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 7\n",
            "Train | Loss:0.40970 Acc: 90.125\n",
            "Valid | Loss:0.40796 Acc: 90.220 \n",
            "saving model with acc 90.220\n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 8\n",
            "Train | Loss:0.41136 Acc: 89.917\n",
            "Valid | Loss:0.40984 Acc: 90.029 \n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 9\n",
            "Train | Loss:0.40749 Acc: 90.311\n",
            "Valid | Loss:0.41289 Acc: 89.701 \n",
            "-----------------------------------------------\n",
            "\n",
            "epoch: 10\n",
            "Train | Loss:0.40785 Acc: 90.303\n",
            "Valid | Loss:0.41004 Acc: 89.934 \n",
            "-----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHFkXiWiXfTH"
      },
      "source": [
        "# fod idx in \n",
        "df[47] = df[47].replace(0,2)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "0yOL0XqkYtxX",
        "outputId": "0e370b1e-52ef-4f35-8c03-f16cd9d1611f"
      },
      "source": [
        "df"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            0       1       2       3       4       5       6       7   \\\n",
              "289320   71119  151133   71120   14476    9147  267911  267911  267911   \n",
              "81076     8352      14     205     561     641      38  267911  267911   \n",
              "232075     150    1941     374  267911  267911  267911  267911  267911   \n",
              "116559      14    1305   83753     956      38  267911  267911  267911   \n",
              "179568   94043  127877  267911  267911  267911  267911  267911  267911   \n",
              "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "367726   17078    1169  267911  267911  267911  267911  267911  267911   \n",
              "367727   17078   56132  267911  267911  267911  267911  267911  267911   \n",
              "367728  242056  267911  267911  267911  267911  267911  267911  267911   \n",
              "367729    2237    1038  267911  267911  267911  267911  267911  267911   \n",
              "367730   19476  267911  267911  267911  267911  267911  267911  267911   \n",
              "\n",
              "            8       9   ...      38      39      40      41      42      43  \\\n",
              "289320  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "81076   267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "232075  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "116559  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "179568  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "...        ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
              "367726  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "367727  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "367728  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "367729  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "367730  267911  267911  ...  267911  267911  267911  267911  267911  267911   \n",
              "\n",
              "            44      45      46  47  \n",
              "289320  267911  267911  267911   2  \n",
              "81076   267911  267911  267911   2  \n",
              "232075  267911  267911  267911   2  \n",
              "116559  267911  267911  267911   2  \n",
              "179568  267911  267911  267911   2  \n",
              "...        ...     ...     ...  ..  \n",
              "367726  267911  267911  267911   2  \n",
              "367727  267911  267911  267911   2  \n",
              "367728  267911  267911  267911   2  \n",
              "367729  267911  267911  267911   2  \n",
              "367730  267911  267911  267911   2  \n",
              "\n",
              "[143273 rows x 48 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>289320</th>\n",
              "      <td>71119</td>\n",
              "      <td>151133</td>\n",
              "      <td>71120</td>\n",
              "      <td>14476</td>\n",
              "      <td>9147</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81076</th>\n",
              "      <td>8352</td>\n",
              "      <td>14</td>\n",
              "      <td>205</td>\n",
              "      <td>561</td>\n",
              "      <td>641</td>\n",
              "      <td>38</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232075</th>\n",
              "      <td>150</td>\n",
              "      <td>1941</td>\n",
              "      <td>374</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116559</th>\n",
              "      <td>14</td>\n",
              "      <td>1305</td>\n",
              "      <td>83753</td>\n",
              "      <td>956</td>\n",
              "      <td>38</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179568</th>\n",
              "      <td>94043</td>\n",
              "      <td>127877</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367726</th>\n",
              "      <td>17078</td>\n",
              "      <td>1169</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367727</th>\n",
              "      <td>17078</td>\n",
              "      <td>56132</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367728</th>\n",
              "      <td>242056</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367729</th>\n",
              "      <td>2237</td>\n",
              "      <td>1038</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367730</th>\n",
              "      <td>19476</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>...</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>267911</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>143273 rows × 48 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO1d-NrhZQjs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "57078412-6913-408f-8650-9235b93dd83a"
      },
      "source": [
        "outputs_class = []\n",
        "for i in range(outputs.shape[0]):\n",
        "  if outputs[i,0] > outputs[i,1]:\n",
        "    outputs_class.append(0)\n",
        "  else:\n",
        "    outputs_class.append(1)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-63-fb1f4d980c11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moutputs_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0moutputs_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzbqdAKGaz-B"
      },
      "source": [
        "cm = confusion_matrix(outputs_class, train_loader.dataset.label.cpu())\n",
        "print(cm)\n",
        "print(accuracy_score(outputs_class, train_loader.dataset.label.cpu()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75tM_ccr4feu"
      },
      "source": [
        "sum = 0\n",
        "for y in y_train:\n",
        "  if y == 1:\n",
        "    sum += 1\n",
        "print(sum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZFTjAZYVy2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d64c80-a434-4c8c-d3b0-289fc4fdbcee"
      },
      "source": [
        "# Find the closer word\n",
        "preprocess.embedding.wv.closer_than('厲害','好棒')"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['也',\n",
              " '不',\n",
              " '真的',\n",
              " '讓',\n",
              " '想',\n",
              " '什麼',\n",
              " '去',\n",
              " '做',\n",
              " '多',\n",
              " '話',\n",
              " '喜歡',\n",
              " '事情',\n",
              " '事',\n",
              " '難過',\n",
              " '一樣',\n",
              " '一次',\n",
              " '像',\n",
              " '很多',\n",
              " '一點',\n",
              " '哭',\n",
              " '開心',\n",
              " '幫',\n",
              " '大',\n",
              " '需要',\n",
              " '.....',\n",
              " '問題',\n",
              " '懂',\n",
              " '最近',\n",
              " '錯',\n",
              " '痛',\n",
              " '打',\n",
              " '感謝',\n",
              " '不好',\n",
              " '長',\n",
              " '起來',\n",
              " '痛苦',\n",
              " '電話',\n",
              " '恨',\n",
              " '這次',\n",
              " '認識',\n",
              " '明明',\n",
              " '累',\n",
              " 'my',\n",
              " '重要',\n",
              " '英文',\n",
              " '唉',\n",
              " '難',\n",
              " '不用',\n",
              " '祝福',\n",
              " '2',\n",
              " '身體',\n",
              " '討厭',\n",
              " '女生',\n",
              " '付出',\n",
              " '擔心',\n",
              " '第一次',\n",
              " '捨',\n",
              " '玩',\n",
              " '真正',\n",
              " '了解',\n",
              " '清楚',\n",
              " '站',\n",
              " '出去',\n",
              " '騙',\n",
              " '健康',\n",
              " '不夠',\n",
              " '這裡',\n",
              " '壓力',\n",
              " '存在',\n",
              " '身上',\n",
              " '男人',\n",
              " '忙',\n",
              " '真心',\n",
              " '堅強',\n",
              " '聊天',\n",
              " '辛苦',\n",
              " '除了',\n",
              " '住',\n",
              " '容易',\n",
              " '方式',\n",
              " '值得',\n",
              " '在乎',\n",
              " '簡單',\n",
              " '自私',\n",
              " '見面',\n",
              " '回去',\n",
              " '聯絡',\n",
              " '男生',\n",
              " '天氣',\n",
              " '幫忙',\n",
              " '女朋友',\n",
              " '遠',\n",
              " '啥',\n",
              " '一開始',\n",
              " '受傷',\n",
              " '平安',\n",
              " '傻',\n",
              " '吵架',\n",
              " '字',\n",
              " '當下',\n",
              " '輕',\n",
              " 'wretch',\n",
              " '面前',\n",
              " 'sorry',\n",
              " '待',\n",
              " '懂得',\n",
              " '在意',\n",
              " '文章',\n",
              " '差',\n",
              " '靠',\n",
              " '因此',\n",
              " '兒子',\n",
              " '可惜',\n",
              " '成熟',\n",
              " '自以',\n",
              " '還好',\n",
              " '個性',\n",
              " '不行',\n",
              " '比較',\n",
              " '害',\n",
              " '沒事',\n",
              " '穿',\n",
              " '過的',\n",
              " '浪費',\n",
              " '打給',\n",
              " 'XD',\n",
              " '高興',\n",
              " '在家',\n",
              " '沒用',\n",
              " '祝',\n",
              " '自',\n",
              " '失敗',\n",
              " '失戀',\n",
              " '高',\n",
              " '弄',\n",
              " '徹底',\n",
              " '冷',\n",
              " '講話',\n",
              " '令人',\n",
              " '那邊',\n",
              " 'imgur',\n",
              " '單純',\n",
              " '喝',\n",
              " '幾次',\n",
              " '不錯',\n",
              " '舒服',\n",
              " '笨',\n",
              " '果然',\n",
              " '保護',\n",
              " '深',\n",
              " '貼',\n",
              " '欺騙',\n",
              " '過程',\n",
              " '版',\n",
              " '外面',\n",
              " '一般',\n",
              " '哈哈',\n",
              " 'po',\n",
              " '並且',\n",
              " '奇怪',\n",
              " '溝通',\n",
              " '嚴重',\n",
              " '此',\n",
              " '大哭',\n",
              " '嘴',\n",
              " '殺',\n",
              " '真實',\n",
              " '難受',\n",
              " '緊張',\n",
              " '沒人',\n",
              " '流淚',\n",
              " '算',\n",
              " '表現',\n",
              " '可怕',\n",
              " '出去玩',\n",
              " '立刻',\n",
              " '內容',\n",
              " '線',\n",
              " '獨立',\n",
              " '沉默',\n",
              " '癡',\n",
              " '一面',\n",
              " '脆弱',\n",
              " '疼',\n",
              " '用心',\n",
              " '幼稚',\n",
              " '可憐',\n",
              " '糟糕',\n",
              " '不理',\n",
              " '有用',\n",
              " '會不會',\n",
              " '心疼',\n",
              " '總覺',\n",
              " '讚',\n",
              " '選舉',\n",
              " '挺',\n",
              " '一張',\n",
              " '連絡',\n",
              " '鬼',\n",
              " '無所謂',\n",
              " '呼吸',\n",
              " '心態',\n",
              " '上次',\n",
              " '腳',\n",
              " '沒錯',\n",
              " '笨蛋',\n",
              " '多次',\n",
              " '考慮',\n",
              " '進去',\n",
              " '兇',\n",
              " '負責',\n",
              " '網路',\n",
              " '重點',\n",
              " '正',\n",
              " '急',\n",
              " '灣',\n",
              " '放心',\n",
              " '拍',\n",
              " '折磨',\n",
              " '票',\n",
              " '謊',\n",
              " '公平',\n",
              " '惹',\n",
              " '不爽',\n",
              " '輸',\n",
              " '媒體',\n",
              " '可笑',\n",
              " '還說',\n",
              " '狠狠',\n",
              " '滿足',\n",
              " '下定',\n",
              " 'X',\n",
              " '瘋',\n",
              " '理性',\n",
              " '欠',\n",
              " '殘忍',\n",
              " '傻傻的',\n",
              " '還以',\n",
              " '矛盾',\n",
              " '大聲',\n",
              " '盡力',\n",
              " '好吃',\n",
              " '條件',\n",
              " '無情',\n",
              " '文',\n",
              " '姐姐',\n",
              " '假日',\n",
              " '爬',\n",
              " '瘋狂',\n",
              " '低',\n",
              " '08',\n",
              " '賤',\n",
              " '勉強',\n",
              " '漂亮',\n",
              " 'youtube',\n",
              " '硬',\n",
              " '可悲',\n",
              " '一聲',\n",
              " '哈',\n",
              " '深刻',\n",
              " '當個',\n",
              " '程度',\n",
              " '不在意',\n",
              " '帽子',\n",
              " 'bbs',\n",
              " '不去',\n",
              " '一通',\n",
              " '幸運',\n",
              " '病',\n",
              " '安全',\n",
              " '標準',\n",
              " 'PTT',\n",
              " '愚蠢',\n",
              " '認同',\n",
              " '似的',\n",
              " '老實',\n",
              " '活著',\n",
              " '部分',\n",
              " '好煩',\n",
              " '善良',\n",
              " '一篇',\n",
              " '髒',\n",
              " '懦弱',\n",
              " '新年',\n",
              " '用力',\n",
              " '瞭解',\n",
              " '很痛',\n",
              " '說話',\n",
              " '比賽',\n",
              " '充實',\n",
              " '開車',\n",
              " '當天',\n",
              " '砍',\n",
              " '表面',\n",
              " '不給',\n",
              " '晚安',\n",
              " '耍',\n",
              " '喝酒',\n",
              " '回報',\n",
              " '這時候',\n",
              " '隔',\n",
              " '扯',\n",
              " 'D',\n",
              " '上線',\n",
              " '很棒',\n",
              " 'ptt',\n",
              " '聰明',\n",
              " '看清',\n",
              " '方便',\n",
              " '勸',\n",
              " '救',\n",
              " '愛他',\n",
              " '狠心',\n",
              " '互動',\n",
              " '白痴',\n",
              " '空氣',\n",
              " '不怕',\n",
              " '想不到',\n",
              " '正確',\n",
              " '利益',\n",
              " '念書',\n",
              " '蠢',\n",
              " '平平安安',\n",
              " '24',\n",
              " '殘酷',\n",
              " '節目',\n",
              " '刪文',\n",
              " '一整',\n",
              " '好笑',\n",
              " '無關',\n",
              " '悲哀',\n",
              " '吸引',\n",
              " '平凡',\n",
              " '不久',\n",
              " '一旦',\n",
              " '記者',\n",
              " '仔細',\n",
              " '尾',\n",
              " '配合',\n",
              " '不合',\n",
              " '不足',\n",
              " '可惡',\n",
              " '軟弱',\n",
              " '中心',\n",
              " 'disp',\n",
              " '廢',\n",
              " '中國人',\n",
              " '會過',\n",
              " '屁',\n",
              " '這兩個',\n",
              " '請用',\n",
              " '酸',\n",
              " '丟臉',\n",
              " '人物',\n",
              " '欺負',\n",
              " '好看',\n",
              " '符合',\n",
              " '外表',\n",
              " '歡迎',\n",
              " '死心',\n",
              " '一遍',\n",
              " '事事',\n",
              " '幹麻',\n",
              " '遠遠',\n",
              " '挑',\n",
              " '有趣',\n",
              " 'goo',\n",
              " '學妹',\n",
              " '這陣子',\n",
              " '熱心',\n",
              " '便宜',\n",
              " '人會',\n",
              " '學著',\n",
              " '傷人',\n",
              " 'facebook',\n",
              " '低落',\n",
              " '還不夠',\n",
              " '抱著',\n",
              " '專業',\n",
              " '謝',\n",
              " '加',\n",
              " '面子',\n",
              " '貪心',\n",
              " 'reader',\n",
              " '一眼',\n",
              " '情形',\n",
              " '發文',\n",
              " '公開',\n",
              " '我連',\n",
              " '勁',\n",
              " '魔',\n",
              " 'XX',\n",
              " '危險',\n",
              " '抱抱',\n",
              " '以外',\n",
              " '智慧',\n",
              " '圖',\n",
              " '不順',\n",
              " '嚇到',\n",
              " '心頭',\n",
              " '近況',\n",
              " '痛哭',\n",
              " 'p',\n",
              " '恐怖',\n",
              " '悶悶',\n",
              " '一刀',\n",
              " '逃',\n",
              " '卑微',\n",
              " '上去',\n",
              " '自尊',\n",
              " '鄉民',\n",
              " '一夜',\n",
              " '快速',\n",
              " '爆卦',\n",
              " '乖',\n",
              " '摟',\n",
              " '自責',\n",
              " '誇張',\n",
              " '旅遊',\n",
              " '打球',\n",
              " '邏輯',\n",
              " '滋味',\n",
              " '舔',\n",
              " '熟',\n",
              " 'android',\n",
              " '棒',\n",
              " '交通',\n",
              " '慌',\n",
              " '好玩',\n",
              " '一部',\n",
              " '瞎',\n",
              " '做什麼',\n",
              " '選前',\n",
              " '好想哭',\n",
              " '這幾個',\n",
              " '很傻',\n",
              " '好奇',\n",
              " '民調',\n",
              " '順',\n",
              " '外套',\n",
              " '不吃',\n",
              " '不痛',\n",
              " '無話',\n",
              " '清晰',\n",
              " '開朗',\n",
              " '打電話給',\n",
              " '驚訝',\n",
              " '頗',\n",
              " '我愛的',\n",
              " '屎',\n",
              " '就連',\n",
              " '交到',\n",
              " '沒資格',\n",
              " '出生',\n",
              " '期末',\n",
              " '光光',\n",
              " '生意',\n",
              " '一廂',\n",
              " '無恥',\n",
              " '狠',\n",
              " '下雨天',\n",
              " '設計',\n",
              " '早起',\n",
              " '分析',\n",
              " '掩飾',\n",
              " '吃醋',\n",
              " 'youtu',\n",
              " '上網',\n",
              " '學到',\n",
              " '做錯',\n",
              " '開刀',\n",
              " '害羞',\n",
              " '課程',\n",
              " '發揮',\n",
              " '當面',\n",
              " '活下去',\n",
              " '看不起',\n",
              " '良好',\n",
              " '心願',\n",
              " '親密',\n",
              " '還不錯',\n",
              " '胸口',\n",
              " '免費',\n",
              " '那次',\n",
              " '想太多',\n",
              " '康',\n",
              " '歡',\n",
              " '負責任',\n",
              " '摔',\n",
              " '實話',\n",
              " '有力',\n",
              " '爛',\n",
              " '很爛',\n",
              " '住院',\n",
              " '鄰居',\n",
              " '偉大',\n",
              " '放在心上',\n",
              " '哩',\n",
              " '試試',\n",
              " '這都',\n",
              " '大方',\n",
              " '自卑',\n",
              " '面具',\n",
              " '插',\n",
              " '寬',\n",
              " '還指',\n",
              " '9.2',\n",
              " '西',\n",
              " '訴說',\n",
              " '剩綠',\n",
              " '做人',\n",
              " '操',\n",
              " '不熟',\n",
              " '誠意',\n",
              " 'ok',\n",
              " '知識',\n",
              " '優秀',\n",
              " '還蠻',\n",
              " '當我們',\n",
              " '發生過',\n",
              " '怨恨',\n",
              " '一看',\n",
              " '缺乏',\n",
              " '要死',\n",
              " '貴',\n",
              " '黏',\n",
              " '扶',\n",
              " '時機',\n",
              " '還幫',\n",
              " '傻眼',\n",
              " '痛恨',\n",
              " '專注',\n",
              " '狼狽',\n",
              " '金榜',\n",
              " '縣市',\n",
              " '迷路',\n",
              " '太好了',\n",
              " '跟別',\n",
              " '族群',\n",
              " '官',\n",
              " '禮貌',\n",
              " '田地',\n",
              " '安',\n",
              " '操作',\n",
              " '提升',\n",
              " '很差',\n",
              " '不好受',\n",
              " '連個',\n",
              " '介意',\n",
              " '基礎',\n",
              " '開會',\n",
              " '親切',\n",
              " '訓練',\n",
              " 'XDD',\n",
              " '玩玩',\n",
              " '平時',\n",
              " '蚊子',\n",
              " '美食',\n",
              " '做個',\n",
              " 'ltn',\n",
              " '玩弄',\n",
              " '一路上',\n",
              " '誠心',\n",
              " '不孝',\n",
              " '評論',\n",
              " '差勁',\n",
              " '產業',\n",
              " 'S',\n",
              " '談話',\n",
              " '伸手',\n",
              " '拜拜',\n",
              " '宅',\n",
              " '房東',\n",
              " '敏感',\n",
              " '拍謝',\n",
              " '科目',\n",
              " '風向',\n",
              " '中立',\n",
              " '灑脫',\n",
              " '鹹',\n",
              " '起碼',\n",
              " '喝醉',\n",
              " '網站',\n",
              " '我傷',\n",
              " '半天',\n",
              " '催眠',\n",
              " 'HatePolitics',\n",
              " 'ID',\n",
              " '幣',\n",
              " '過份',\n",
              " '很會',\n",
              " '合理',\n",
              " '愛到',\n",
              " '跌',\n",
              " '了不起',\n",
              " '合法',\n",
              " '太笨',\n",
              " '某種',\n",
              " '夢境',\n",
              " '鬥爭',\n",
              " '吃藥',\n",
              " '主',\n",
              " '懷孕',\n",
              " '埋怨',\n",
              " '對話',\n",
              " '厚',\n",
              " '經歷過',\n",
              " '晚睡',\n",
              " '客氣',\n",
              " '討好',\n",
              " '腦子',\n",
              " '聊聊天',\n",
              " '怪怪的',\n",
              " '看出',\n",
              " '相似',\n",
              " '考量',\n",
              " '有把握',\n",
              " '冰',\n",
              " '打到',\n",
              " '騙子',\n",
              " '香',\n",
              " '伯伯',\n",
              " '疏遠',\n",
              " '證照',\n",
              " '隊',\n",
              " '正好',\n",
              " '心寒',\n",
              " 'wish',\n",
              " '敗',\n",
              " '假象',\n",
              " '】',\n",
              " '厭煩',\n",
              " '密碼',\n",
              " '累累',\n",
              " '好笨',\n",
              " '閱讀',\n",
              " '心甘',\n",
              " '一到',\n",
              " '痛過',\n",
              " '人數',\n",
              " '屌',\n",
              " '忽視',\n",
              " '視',\n",
              " '根據',\n",
              " '水準',\n",
              " '偷吃',\n",
              " '扶持',\n",
              " 'market',\n",
              " '刻骨',\n",
              " '幽默',\n",
              " '配不上',\n",
              " '倒楣',\n",
              " '有別',\n",
              " '愛哭',\n",
              " '強顏歡',\n",
              " '政黑',\n",
              " '錯覺',\n",
              " '待遇',\n",
              " '電話裡',\n",
              " '星座',\n",
              " '自欺欺人',\n",
              " '之處',\n",
              " '沒聯絡',\n",
              " '寵物',\n",
              " '故作',\n",
              " '瞧不起',\n",
              " '癢',\n",
              " '沒接',\n",
              " '包裝',\n",
              " '例子',\n",
              " '看不懂',\n",
              " '弱',\n",
              " '高分',\n",
              " '狗屁',\n",
              " 'CC',\n",
              " '檢舉',\n",
              " '傳統',\n",
              " '弱勢',\n",
              " '粗心',\n",
              " '功能',\n",
              " '真有',\n",
              " '尿',\n",
              " '混蛋',\n",
              " '都還',\n",
              " '雨天',\n",
              " '親近',\n",
              " '有情',\n",
              " '不易',\n",
              " '同志',\n",
              " '軍宅',\n",
              " '空心',\n",
              " '渺小',\n",
              " '牛',\n",
              " '心聲',\n",
              " '處境',\n",
              " '低調',\n",
              " '玻璃',\n",
              " '三餐',\n",
              " '佩服',\n",
              " '屁話',\n",
              " '紛紛',\n",
              " '捅',\n",
              " '對立',\n",
              " '夜店',\n",
              " '屁事',\n",
              " '很漂亮',\n",
              " '本質',\n",
              " '今以',\n",
              " '打扮',\n",
              " '率',\n",
              " '絕情',\n",
              " '慈悲',\n",
              " '老鼠',\n",
              " '商品',\n",
              " '通話',\n",
              " '沒時間',\n",
              " '變態',\n",
              " '縫',\n",
              " '打臉',\n",
              " '第幾次',\n",
              " '說實',\n",
              " '魅力',\n",
              " '以後還',\n",
              " '很氣',\n",
              " '顫',\n",
              " '看得出',\n",
              " 'its',\n",
              " '栽種',\n",
              " '膽小',\n",
              " '貴人',\n",
              " '全台灣',\n",
              " '打槍',\n",
              " '嫉妒',\n",
              " '警告',\n",
              " '愛玩',\n",
              " '刁',\n",
              " '逛逛',\n",
              " '這間',\n",
              " '得意',\n",
              " '糗',\n",
              " '謊話',\n",
              " '挨',\n",
              " '幹幹',\n",
              " '愛吃',\n",
              " '嚐',\n",
              " '瘋子',\n",
              " '智商',\n",
              " '神經病',\n",
              " 'Thu',\n",
              " '操心',\n",
              " '大力',\n",
              " '沒救',\n",
              " '大腿',\n",
              " '私底下',\n",
              " '上來',\n",
              " '左手',\n",
              " '政績',\n",
              " '沒給',\n",
              " '看不下去',\n",
              " '新鮮',\n",
              " '過敏',\n",
              " '擺爛',\n",
              " '灣不會',\n",
              " '很強',\n",
              " '友善',\n",
              " '滑',\n",
              " 'appledaily',\n",
              " '議會',\n",
              " '團結',\n",
              " '遇過',\n",
              " '關掉',\n",
              " '買到',\n",
              " '接電話',\n",
              " '棒棒',\n",
              " '炫耀',\n",
              " '穩穩',\n",
              " 'tinyurl',\n",
              " '手法',\n",
              " '不能自己',\n",
              " 'Hate',\n",
              " '神奇',\n",
              " '傷害過',\n",
              " '越想',\n",
              " '茁壯',\n",
              " '最多',\n",
              " '談心',\n",
              " '沒人要',\n",
              " '合適',\n",
              " '虛假',\n",
              " '排斥',\n",
              " '沒回',\n",
              " '許還',\n",
              " '忽熱',\n",
              " '口罩',\n",
              " '軍人',\n",
              " '大好人',\n",
              " '技巧',\n",
              " '信賴',\n",
              " '踏實',\n",
              " '不服',\n",
              " '別想',\n",
              " '詳細',\n",
              " '這事',\n",
              " '玩遊戲',\n",
              " 'L',\n",
              " '本以',\n",
              " '弟妹',\n",
              " '打字',\n",
              " '熱鬧',\n",
              " '心機',\n",
              " '反串',\n",
              " '落差',\n",
              " '地板',\n",
              " '掛掉',\n",
              " '不會痛',\n",
              " '害人',\n",
              " '邪',\n",
              " '即可',\n",
              " '可貴',\n",
              " '類型',\n",
              " '淪家',\n",
              " '不得了',\n",
              " 'pixnet',\n",
              " '如題',\n",
              " '汗',\n",
              " '取暖',\n",
              " '頸',\n",
              " 'N',\n",
              " '選項',\n",
              " '意志力',\n",
              " '牛奶',\n",
              " 'Sun',\n",
              " '冷水',\n",
              " '認輸',\n",
              " 'udn',\n",
              " '出包',\n",
              " '風度',\n",
              " '好多年',\n",
              " '無敵',\n",
              " '我妹',\n",
              " '評價',\n",
              " '吃屎',\n",
              " '贏了',\n",
              " '囂張',\n",
              " '近期',\n",
              " '救災',\n",
              " '衰',\n",
              " '想得到',\n",
              " '花心',\n",
              " '沒興趣',\n",
              " '制約',\n",
              " '聯誼',\n",
              " '部長',\n",
              " '維',\n",
              " 'SV',\n",
              " '微不足道',\n",
              " '沒見面',\n",
              " '欣慰',\n",
              " '求救',\n",
              " '有名',\n",
              " '很愛很愛',\n",
              " '流眼淚',\n",
              " '關起',\n",
              " '測驗',\n",
              " '先去',\n",
              " '例外',\n",
              " '生物',\n",
              " '權貴',\n",
              " '很爽',\n",
              " '考得',\n",
              " '痛好',\n",
              " '治',\n",
              " '作秀',\n",
              " 'godshenyarticle',\n",
              " '給我們',\n",
              " '嚴肅',\n",
              " 'EQ',\n",
              " '發自內心',\n",
              " '敞開',\n",
              " '可靠',\n",
              " '摔車',\n",
              " '這跟',\n",
              " '年輕',\n",
              " '做點',\n",
              " '人前',\n",
              " '基層',\n",
              " '變胖',\n",
              " '叛逆',\n",
              " '卑鄙',\n",
              " '好氣',\n",
              " '重重',\n",
              " '親親',\n",
              " '核心',\n",
              " '喲',\n",
              " '自助',\n",
              " '科',\n",
              " '反倒',\n",
              " '心傷',\n",
              " '筆試',\n",
              " '那現',\n",
              " '小心眼',\n",
              " '嚴格',\n",
              " '一會',\n",
              " '不便',\n",
              " '優',\n",
              " '狗屎',\n",
              " '偏激',\n",
              " '有意思',\n",
              " '字句',\n",
              " '總比',\n",
              " '優柔',\n",
              " '我門',\n",
              " '打牌',\n",
              " '41',\n",
              " '低能',\n",
              " '日日',\n",
              " '開懷',\n",
              " '這並',\n",
              " '一開',\n",
              " '另一方面',\n",
              " '做不了',\n",
              " '寵壞',\n",
              " '去給',\n",
              " '帥',\n",
              " '收費',\n",
              " '外國人',\n",
              " '褲',\n",
              " '底線',\n",
              " '影響力',\n",
              " '胎',\n",
              " '聯合報',\n",
              " '幾張',\n",
              " '順手',\n",
              " '冷酷',\n",
              " 'dear',\n",
              " '球場',\n",
              " '早睡',\n",
              " '痛痛',\n",
              " '玻璃心',\n",
              " '陪你去',\n",
              " '不愧',\n",
              " '高手',\n",
              " '客觀',\n",
              " '可恥',\n",
              " '自作自受',\n",
              " '成就感',\n",
              " '嚴厲',\n",
              " '待人',\n",
              " '可恨',\n",
              " '面對面',\n",
              " '擔當',\n",
              " '薄弱',\n",
              " '命令',\n",
              " '兩黨',\n",
              " '希拉',\n",
              " '蕊',\n",
              " '討人厭',\n",
              " '姓',\n",
              " '鬧鬧',\n",
              " '百分百',\n",
              " '不識',\n",
              " '嘩',\n",
              " '駕駛',\n",
              " 'high',\n",
              " '這有',\n",
              " '直白',\n",
              " '行業',\n",
              " '線上',\n",
              " '出面',\n",
              " '礁',\n",
              " '護士',\n",
              " '大包',\n",
              " '洗面',\n",
              " '越南',\n",
              " '麻醉',\n",
              " '隱形',\n",
              " '個爛',\n",
              " '見識',\n",
              " '退步',\n",
              " '盡心',\n",
              " '口是心非',\n",
              " '致命',\n",
              " '十足',\n",
              " '人給',\n",
              " 'nothing',\n",
              " '何在',\n",
              " '仁慈',\n",
              " '交了',\n",
              " '挑剔',\n",
              " '鏡',\n",
              " '沒愛過',\n",
              " '渺茫',\n",
              " '豁達',\n",
              " '皇帝',\n",
              " '沒種',\n",
              " '隨心',\n",
              " '雪',\n",
              " '悲情',\n",
              " '商人',\n",
              " '詭異',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_n-Oad3x1FG"
      },
      "source": [
        "train_loader.dataset.label[0:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fQeaQNeNm3L"
      },
      "source": [
        "### Predict and Write to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFvjFQopxVrt"
      },
      "source": [
        "# # 開始測試模型並做預測\n",
        "# print(\"loading testing data ...\")\n",
        "# test_x_word = load_testing_data(testing_data)\n",
        "# preprocess = Preprocess(test_x_word, sen_len, w2v_path=w2v_path)\n",
        "# embedding = preprocess.make_embedding(load=True)\n",
        "# test_x = preprocess.sentence_word2idx()\n",
        "# test_dataset = CustomDataset(X=test_x, y=None)\n",
        "# test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "#                                             batch_size = batch_size,\n",
        "#                                             shuffle = False,\n",
        "#                                             num_workers = 8)\n",
        "# print('\\nload model ...')\n",
        "# model = torch.load(os.path.join(model_dir, 'ckpt-Kao.model'))\n",
        "# outputs = testing(batch_size, test_loader, model, device)\n",
        "\n",
        "# # 寫到 csv 檔案供上傳 Kaggle\n",
        "# tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\n",
        "# print(\"save csv ...\")\n",
        "# tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
        "# print(\"Finish Predicting\")\n",
        "\n",
        "# # 以下是使用 command line 上傳到 Kaggle 的方式\n",
        "# # 需要先 pip install kaggle、Create API Token，詳細請看 https://github.com/Kaggle/kaggle-api 以及 https://www.kaggle.com/code1110/how-to-submit-from-google-colab\n",
        "# # kaggle competitions submit [competition-name] -f [csv file path]] -m [message]\n",
        "# # e.g., kaggle competitions submit ml-2020spring-hw4 -f output/predict.csv -m \"......\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV1s4W_aWlvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77a7d6d3-8712-480f-fff0-5f3ada6b4693"
      },
      "source": [
        "preprocess = Preprocess([['輝煌']], sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = CustomDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                        batch_size = batch_size,\n",
        "                        shuffle = False)\n",
        "                        # num_workers = 8)\n",
        "print('\\nload model ...')\n",
        "model = torch.load(os.path.join(model_dir, 'ckpt-NTCH.model'))\n",
        "outputs = testing(batch_size, test_loader, model, device)\n",
        "print(outputs)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #267911\n",
            "total words: 267913\n",
            "sentence count #1\n",
            "load model ...\n",
            "[tensor([9.9980e-01, 1.7410e-06], device='cuda:0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZf3E2O1wsQo"
      },
      "source": [
        "#### Download the files to your computer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzsAmmRUwqdA"
      },
      "source": [
        "# from google.colab import files\n",
        "# files.download('predict.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}